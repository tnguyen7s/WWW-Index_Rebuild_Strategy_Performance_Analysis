{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read excel_file \n",
    "def read_file(file_name):\n",
    "    file = pd.ExcelFile(file_name)\n",
    "    list = []\n",
    "    \n",
    "    #reading data in each sheet of the file and storing in a list\n",
    "    for sheet_name in file.sheet_names:\n",
    "        df = pd.read_excel(file, sheet_name = sheet_name)\n",
    "        #averaging the values to the minute granularity \n",
    "        df['Date'] = pd.to_datetime(df['Date (UTC)']).dt.floor('min')\n",
    "        df = df.drop(columns = ['Date (UTC)'])\n",
    "        average_min = df.groupby('Date').mean()\n",
    "        average_min = average_min.reset_index()\n",
    "        average_min['Date'] = pd.to_datetime(average_min['Date'])\n",
    "        list.append(average_min)\n",
    "    return list \n",
    "\n",
    "#function to merge all the sheets' dataframe into one dataframe\n",
    "def merge(list):\n",
    "    #create a dataframe from the list of dataframes\n",
    "    result = list[0]\n",
    "    \n",
    "    #merging all the dataframes based on the common date\n",
    "    for index, data_frame in enumerate (list):\n",
    "        if index < len(list) - 1:\n",
    "            result = result.merge(list[index + 1], on = 'Date', how = 'inner')\n",
    "    \n",
    "    #changing columns' name\n",
    "    new_cols = {}\n",
    "    for column in result.columns:\n",
    "        new_column = column.replace(' for PRODSQL\\\\LOCAL','').replace('SQL Server: ','').replace(' ','_').lower()\n",
    "        new_cols[column] = new_column\n",
    "    new_cols['Disk avg. read time for prodsql1-vm.wwwoodproducts.com > M: (SQL Data 1)'] = 'disk_avg_read_time_1'\n",
    "    new_cols['Disk avg. read time for prodsql1-vm.wwwoodproducts.com > N: (SQL Data 2)'] = 'disk_avg_read_time_2'\n",
    "    new_cols['Disk avg. read time for prodsql1-vm.wwwoodproducts.com > I: (SQL Index)'] = 'index1'\n",
    "    new_cols['Disk avg. write time for prodsql1-vm.wwwoodproducts.com > M: (SQL Data 1)'] = 'disk_avg_write_time_1'\n",
    "    new_cols['Disk avg. write time for prodsql1-vm.wwwoodproducts.com > N: (SQL Data 2)'] = 'disk_avg_write_time_2'\n",
    "    new_cols['Disk avg. write time for prodsql1-vm.wwwoodproducts.com > I: (SQL Index)'] = 'index2'\n",
    "    result.rename(columns = new_cols, inplace = True )\n",
    "    \n",
    "    return result\n",
    "\n",
    "#function to handling data\n",
    "def data_process(data_frame):\n",
    "    #converting the Date (UTC) to Date(CST)\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    utc_timezone = pytz.utc\n",
    "    cst_timezone = pytz.timezone('America/Chicago')\n",
    "    data_frame['date'] = data_frame['date'].dt.tz_localize('UTC').dt.tz_convert(cst_timezone)\n",
    "    \n",
    "    #converting Free Memory metric from Bytes to GB\n",
    "    data_frame['free_memory'] = data_frame['free_memory']/1024**3\n",
    "    \n",
    "    #Multiplying the compilations/batch by 100\n",
    "    data_frame['compilations/batch'] = data_frame['compilations/batch']*100\n",
    "    \n",
    "    #Combine Disk avg read time sql data 1 and Disk avg read time sql data 2 into one column Disk avg read time sql data using a SUM\n",
    "    data_frame['disk_avg_read_time_sql_data']  = data_frame['disk_avg_read_time_1']+data_frame['disk_avg_read_time_2']\n",
    "    return data_frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading files, concatenating and writing to csv\n",
    "def reading(file_name):\n",
    "    list_of_file = read_file(file_name)\n",
    "    df = merge(list_of_file)\n",
    "    df = data_process(df)\n",
    "    return df\n",
    "\n",
    "datasetA1 = reading('1st_day_A.xlsx')\n",
    "datasetA2 = reading('2nd_day_A.xlsx')\n",
    "datasetA3 = reading('3rd_day_A.xlsx')\n",
    "datasetA4 = reading('4th_day_A.xlsx')\n",
    "datasetA5 = reading('5th_day_A.xlsx')\n",
    "\n",
    "datasetA = pd.concat([datasetA1,datasetA2,datasetA3,datasetA4,datasetA5])\n",
    "datasetA.to_csv('A.csv')\n",
    "\n",
    "datasetB1 = reading('1st_day_B.xlsx')\n",
    "datasetB2 = reading('2nd_day_B.xlsx')\n",
    "datasetB3 = reading('3rd_day_B.xlsx')\n",
    "datasetB4 = reading('4th_day_B.xlsx')\n",
    "datasetB5 = reading('5th_day_B.xlsx')\n",
    "\n",
    "datasetA = pd.concat([datasetB1,datasetB2,datasetB3,datasetB4,datasetB5],ignore_index= True)\n",
    "datasetA.to_csv('B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7185 entries, 0 to 7184\n",
      "Data columns (total 16 columns):\n",
      " #   Column                       Non-Null Count  Dtype                    \n",
      "---  ------                       --------------  -----                    \n",
      " 0   Unnamed: 0                   7185 non-null   int64                    \n",
      " 1   date                         7185 non-null   datetime64[ns, UTC-05:00]\n",
      " 2   batch_requests/sec           7185 non-null   float64                  \n",
      " 3   user_connections             7185 non-null   float64                  \n",
      " 4   processor_time               7185 non-null   float64                  \n",
      " 5   free_memory                  7185 non-null   float64                  \n",
      " 6   page_reads/sec               7185 non-null   float64                  \n",
      " 7   disk_avg_read_time_1         7185 non-null   float64                  \n",
      " 8   disk_avg_read_time_2         7185 non-null   float64                  \n",
      " 9   index1                       7185 non-null   float64                  \n",
      " 10  disk_avg_write_time_1        7185 non-null   float64                  \n",
      " 11  disk_avg_write_time_2        7185 non-null   float64                  \n",
      " 12  index2                       7185 non-null   float64                  \n",
      " 13  compilations/batch           7185 non-null   float64                  \n",
      " 14  latch_wait_time              7185 non-null   float64                  \n",
      " 15  disk_avg_read_time_sql_data  7185 non-null   float64                  \n",
      "dtypes: datetime64[ns, UTC-05:00](1), float64(14), int64(1)\n",
      "memory usage: 898.3 KB\n"
     ]
    }
   ],
   "source": [
    "#Reading data from csv file and filtering results\n",
    "dataA = pd.read_csv('A.csv')\n",
    "dataB = pd.read_csv('B.csv')\n",
    "\n",
    "dataA['date'] = pd.to_datetime(dataA['date'])\n",
    "dataA_filtered = dataA[(dataA['date'].dt.hour>=5) & (dataA['date'].dt.hour<= 16)]\n",
    "dataA_filtered.to_csv(\"[Daytime]A.csv\", index=False)\n",
    "\n",
    "dataB['date'] = pd.to_datetime(dataB['date'])\n",
    "dataB_filtered = dataB[(dataB['date'].dt.hour>=5) & (dataB['date'].dt.hour<= 16)]\n",
    "dataB_filtered.to_csv(\"[Daytime]B.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
