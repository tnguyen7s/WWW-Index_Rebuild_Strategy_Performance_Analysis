{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processing():\n",
    "\n",
    "  def __init__(self, file_name):\n",
    "    self.file_name = file_name\n",
    "  \n",
    "  def merged_func(self, file_name):\n",
    "    xlsx_file =  pd.ExcelFile(file_name)\n",
    "    dfs = []\n",
    "    for sheet_name in xlsx_file.sheet_names:\n",
    "      df = pd.read_excel(xlsx_file,\n",
    "                         sheet_name=sheet_name)\n",
    "      df = df.set_index('Date (UTC)')\n",
    "      df_resampled = df.resample('min').mean()\\\n",
    "                                     .reset_index()\n",
    "      dfs.append(df_resampled)\n",
    "    \n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right,\n",
    "                                                  on='Date (UTC)', \n",
    "                                                  how='inner' ),dfs)\n",
    "    return merged_df\n",
    "  \n",
    "  def to_CST(self, data):\n",
    "    data['Date (CST)'] = data['Date (UTC)'].dt.tz_localize('utc')\\\n",
    "                                       .dt.tz_convert('US/Central')\\\n",
    "                                       .dt.tz_localize(None)\n",
    "    data = data.set_index('Date (CST)').reset_index()\n",
    "    data = data.drop(columns=['Date (UTC)'])\n",
    "    return data\n",
    "  \n",
    "  \n",
    "  def change_name(self, data):\n",
    "    new_names = {\n",
    "        'Date (CST)': 'DateTime', \n",
    "        'Batch requests/sec for PRODSQL\\LOCAL':'Batch requests/sec',\n",
    "        'User connections for PRODSQL\\LOCAL':'User connections',\n",
    "        'SQL Server: processor time for PRODSQL\\LOCAL':'Processor Time',\n",
    "        'SQL Server: processor time for PRODSQL\\ACCT': 'Processor Time',\n",
    "        'SQL Server: free memory for PRODSQL\\LOCAL':'Free memory',\n",
    "        'Page reads/sec for PRODSQL\\LOCAL':'Page reads/sec',\n",
    "        'Disk avg. read time for prodsql1-vm.wwwoodproducts.com > M: (SQL Data 1)':'Disk avg. read time > M: Data 1',\n",
    "        'Disk avg. read time for prodsql1-vm.wwwoodproducts.com > N: (SQL Data 2)':'Disk avg. read time > N: Data 2',\n",
    "        'Disk avg. read time for prodsql1-vm.wwwoodproducts.com > I: (SQL Index)':'Disk avg. read time > I: Index',\n",
    "        'Disk avg. write time for prodsql1-vm.wwwoodproducts.com > M: (SQL Data 1)':'Disk avg. write time > M: Data 1',\n",
    "        'Disk avg. write time for prodsql1-vm.wwwoodproducts.com > N: (SQL Data 2)':'Disk avg. write time > N: Data 2',\n",
    "        'Disk avg. write time for prodsql1-vm.wwwoodproducts.com > I: (SQL Index)':'Disk avg. write time > I: Index',\n",
    "        'Compilations/batch for PRODSQL\\LOCAL':'Compilations/batch',\n",
    "        'Latch wait time for PRODSQL\\LOCAL':'Latch wait time'\n",
    "       }\n",
    "    \n",
    "    data = data.rename(columns = new_names)\n",
    "    return data\n",
    "  \n",
    "  def set_time(self, data):\n",
    "    data['Time'] = pd.to_datetime(data['DateTime']).dt.time\n",
    "    data = data.set_index('Time').reset_index()\n",
    "    return data\n",
    "  \n",
    "  def set_date(self, data):\n",
    "    data['Date'] = pd.to_datetime(data['DateTime']).dt.date\n",
    "    data = data.set_index('Date').reset_index()\n",
    "    return data\n",
    "  def set_day(self, data):\n",
    "    data['Day_name'] = pd.to_datetime(data['DateTime']).dt.day_name()\n",
    "    day_abbr = {\n",
    "        'Monday': 'Mon',\n",
    "        'Tuesday': 'Tue',\n",
    "        'Wednesday': 'Wed',\n",
    "        'Thursday': 'Thu',\n",
    "        'Friday': 'Fri',\n",
    "        'Saturday': 'Sat',\n",
    "        'Sunday': 'Sun'\n",
    "    }\n",
    "    data['Day_name'] = data['Day_name'].map(day_abbr)\n",
    "    data = data.set_index('Day_name').reset_index()\n",
    "    return data\n",
    "  \n",
    "  def to_GB(self,data):\n",
    "    data['Free memory'] = data['Free memory']/(1024**3)\n",
    "    return data  \n",
    "  \n",
    "  def multiply_100(self,data):\n",
    "    data['Compilations/batch'] = data['Compilations/batch']*100\n",
    "    return data\n",
    "  \n",
    "  def combine_read_time(self,data):\n",
    "    data['Disk avg. read time data'] = data['Disk avg. read time > M: Data 1'] + data['Disk avg. read time > N: Data 2']\n",
    "    data['Disk avg. write time data'] = data['Disk avg. write time > M: Data 1'] + data['Disk avg. write time > N: Data 2']\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "  def run(self):\n",
    "    data =  self.merged_func(self.file_name)\n",
    "    data = self.to_CST(data)\n",
    "    data = self.change_name(data)\n",
    "    data = self.set_time(data)\n",
    "    data = self.set_date(data)\n",
    "    data = self.set_day(data)\n",
    "    data = self.to_GB(data)\n",
    "    data = self.multiply_100(data)\n",
    "    data = self.combine_read_time(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data):\n",
    "    start_time = pd.to_datetime('05:00:00').time()\n",
    "    end_time = pd.to_datetime('16:00:00').time()\n",
    "    data = data[(data['DateTime'].dt.time >= start_time) & (data['DateTime'].dt.time <= end_time)]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processed(A_or_B,filter='full'):\n",
    "  os.makedirs(f'processed_datasets/{filter}data', exist_ok=True)\n",
    "\n",
    "  xlsx_files =  os.listdir(f'datasets/{A_or_B}')\n",
    "\n",
    "  dfs = []\n",
    "  dfs_filtered = []\n",
    "\n",
    "  for xlsx_file in xlsx_files:\n",
    "    data = Processing(f'datasets/{A_or_B}/{xlsx_file}')\n",
    "    data = data.run()\n",
    "    if filter == 'filtered':\n",
    "      dfs_filtered.append(filter_data(data))\n",
    "    dfs.append(data)\n",
    "\n",
    "  if filter == 'filtered':\n",
    "    pd.concat(dfs_filtered).to_csv(f'processed_datasets/{filter}data/{A_or_B}.csv', index=False)     \n",
    "  else:\n",
    "    pd.concat(dfs).to_csv(f'processed_datasets/{filter}data/{A_or_B}.csv', index=False) \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed('A')\n",
    "data_processed('B')\n",
    "data_processed('A', 'filtered')\n",
    "data_processed('B', 'filtered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
