{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processing():\n",
    "\n",
    "  def __init__(self, file_name):\n",
    "    self.file_name = file_name\n",
    "  \n",
    "  def merged_func(self, file_name):\n",
    "    xlsx_file =  pd.ExcelFile(file_name)\n",
    "    dfs = []\n",
    "    for sheet_name in xlsx_file.sheet_names:\n",
    "      df = pd.read_excel(xlsx_file,\n",
    "                         sheet_name=sheet_name)\n",
    "      df = df.set_index('Date (UTC)')\n",
    "      df_resampled = df.resample('min').mean()\\\n",
    "                                     .reset_index()\n",
    "      dfs.append(df_resampled)\n",
    "    \n",
    "    merged_df = reduce(lambda left, right: pd.merge(left, right,\n",
    "                                                  on='Date (UTC)', \n",
    "                                                  how='inner' ),dfs)\n",
    "    return merged_df\n",
    "  \n",
    "  def to_CST(self, data):\n",
    "    data['Date (CST)'] = data['Date (UTC)'].dt.tz_localize('utc')\\\n",
    "                                       .dt.tz_convert('US/Central')\\\n",
    "                                       .dt.tz_localize(None)\n",
    "    data = data.set_index('Date (CST)').reset_index()\n",
    "    return data\n",
    "  \n",
    "  def change_name(self, data):\n",
    "    new_names = {\n",
    "        'Date (CST)': 'Date', \n",
    "        'Batch requests/sec for PRODSQL\\LOCAL':'Batch requests/sec',\n",
    "        'User connections for PRODSQL\\LOCAL':'User connections',\n",
    "        'SQL Server: processor time for PRODSQL\\LOCAL':'Processor Time',\n",
    "        'SQL Server: free memory for PRODSQL\\LOCAL':'Free memory',\n",
    "        'Page reads/sec for PRODSQL\\LOCAL':'Page reads/sec',\n",
    "        'Disk avg. read time for prodsql1-vm.wwwoodproducts.com > M: (SQL Data 1)':'Disk avg. read time > M: Data 1',\n",
    "        'Disk avg. read time for prodsql1-vm.wwwoodproducts.com > N: (SQL Data 2)':'Disk avg. read time > N: Data 2',\n",
    "        'Disk avg. read time for prodsql1-vm.wwwoodproducts.com > I: (SQL Index)':'Disk avg. read time > I: Index',\n",
    "        'Disk avg. write time for prodsql1-vm.wwwoodproducts.com > M: (SQL Data 1)':'Disk avg. write time > M: Data 1',\n",
    "        'Disk avg. write time for prodsql1-vm.wwwoodproducts.com > N: (SQL Data 2)':'Disk avg. write time > N: Data 2',\n",
    "        'Disk avg. write time for prodsql1-vm.wwwoodproducts.com > I: (SQL Index)':'Disk avg. write time > I: Index',\n",
    "        'Compilations/batch for PRODSQL\\LOCAL':'Compilations/batch',\n",
    "        'Latch wait time for PRODSQL\\LOCAL':'Latch wait time'\n",
    "       }\n",
    "    data = data.rename(columns = new_names)\n",
    "    return data\n",
    "  \n",
    "  def to_GB(self,data):\n",
    "    data['Free memory'] = data['Free memory']/(1024**3)\n",
    "    return data  \n",
    "  \n",
    "  def multiply_100(self,data):\n",
    "    data['Compilations/batch'] = data['Compilations/batch']*100\n",
    "    return data\n",
    "  \n",
    "  def combine_read_time(self,data):\n",
    "    data['Disk avg. read time data'] = data['Disk avg. read time > M: Data 1'] + data['Disk avg. read time > N: Data 2']\n",
    "    return data\n",
    "  \n",
    "  def filter_data(self, data):\n",
    "    start_time = pd.to_datetime('05:00:00').time()\n",
    "    end_time = pd.to_datetime('17:00:00').time()\n",
    "    data = data[(data['Date'].dt.time >= start_time) & (data['Date'].dt.time <= end_time)]\n",
    "    return data\n",
    "\n",
    "  def run(self):\n",
    "    data =  self.merged_func(self.file_name)\n",
    "    data = self.to_CST(data)\n",
    "    data = self.change_name(data)\n",
    "    data = self.to_GB(data)\n",
    "    data = self.multiply_100(data)\n",
    "    data = self.combine_read_time(data)\n",
    "    data = self.filter_data(data)\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_processed(A_or_B):\n",
    "  xlsx_files =  os.listdir(f'datasets/{A_or_B}')\n",
    "  for xlsx_file in xlsx_files:\n",
    "    data = Processing(f'datasets/{A_or_B}/{xlsx_file}')\n",
    "    data = data.run()\n",
    "    os.makedirs(f'processed_datasets/{A_or_B}', exist_ok=True)\n",
    "    data.to_csv(f'processed_datasets/{A_or_B}/{xlsx_file}.csv')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed('A')\n",
    "data_processed('B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat(A_or_B):\n",
    "  xlsx_files = os.listdir(f'processed_datasets/{A_or_B}')\n",
    "  dfs = []\n",
    "  for xlsx_file in xlsx_files:\n",
    "    df = pd.read_csv(f'processed_datasets/{A_or_B}/{xlsx_file}')\n",
    "    dfs.append(df)\n",
    "  data = pd.concat(dfs)\n",
    "  data.to_csv(f'processed_datasets/{A_or_B}/full{A_or_B}.csv')\n",
    "concat('A')\n",
    "concat('B')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
